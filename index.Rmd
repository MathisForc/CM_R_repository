---
title: "Analyzing Sub-genres in House Music"
author: "Matthijs Vork"
date: "April 2024"
output: 
  flexdashboard::flex_dashboard:
---

```{r setup, include=FALSE}
remotes::install_github('jaburgoyne/compmus')
library(spotifyr)
library(plotly)
library(tidyverse)
library(tidymodels)
library(usethis)
library(ggplot2)
library(flexdashboard)
library(dplyr)
library(dtw)
library(compmus)
library(ggdendro)
library(heatmaply)
library(cluster)
library(protoclust)

knitr::opts_chunk$set(echo = TRUE)
bass_house <- get_playlist_audio_features("", "2QhCZhrEXekqb13nGvLpHr")
tech_house <- get_playlist_audio_features("", "1XdDSaC9cfzSnPtZpesFsZ")
microhouse <- get_playlist_audio_features("", "71VZsDzDNSNpJluKcZ6x5Q")

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
} 

```

<style>

.chordogram {
  text-align: center;
}

.figures {
  display: flex;
  flex-wrap: wrap;
  justify-content: space-around;
  margin-top: 20px;
}

.figure {
  margin: 0 20px;
  width: 30%;
  height: 100%;
}

.figure img {
  max-width: 100%;
  height: auto;
}

.headr{
  padding: 2% 35% 0% 10%;
  font-size: 3.5vmax;
}

.headr hr{ width:100%;height:100%;background:#000;}

.text {
  padding: 2% 10% 2% 10%;
  font-size: 1.2vmax;
}

.header_wrapper_single{
  padding: 2% 10% 0% 10%;
}

.header_single{
  font-size: 3.5vmax;
}

.text_single{
  padding: 2% 10% 2% 10%;
  font-size: 1.2vmax;
}

.header_question_single{
  font-size: 1.2vmax;
}

.figure_single{
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}

.navbar {
  background-color:black;
  border-color:black;
}

.navbar-inverse {
  font-size: 0.7vw + 0.7vh;
}

.navbar-inverse .navbar-nav>.active > a, .navbar-inverse .navbar-nav>.active > a:hover, .navbar-inverse .navbar-nav>.active > a:focus {
  color: #ffffff;
  background-color: #ff8c00;
}
.navbar-inverse .navbar-nav>li > a:hover, .navbar-inverse .navbar-nav > li > a:focus {
  color: #ffffff;
  background-color: #ffbc6b;    
}

</style>

Introduction {.intro-text}
=====================================
<div class="wrapper">
<div class="headr">
Introduction
</div>
<div class="text">
The selected corpus encompasses three sub-genres: Bass House, Microhouse, and Tech House. Currently, it comprises 128 tracks, with 63 tracks categorized as <a href="https://open.spotify.com/playlist/2QhCZhrEXekqb13nGvLpHr?si=60636996b7d540bf" target="_blank">Bass House</a>, 32 as <a href="https://open.spotify.com/playlist/71VZsDzDNSNpJluKcZ6x5Q?si=fca0039a5bb3437b" target="_blank">Microhouse</a>, and 33 as <a href="https://open.spotify.com/playlist/1XdDSaC9cfzSnPtZpesFsZ?si=b9696110dc9c41eb" target="_blank">Tech House</a>. These genres were chosen due to their ability to span a broad yet distinguishable range within the house music spectrum. The Bass House tracks are derived from compositions by Samuel Deep, Brawther, and Anil Aras. This sub-genre emphasizes the lower spectrum of house music, characterized by powerful kicks, deep basslines, and rhythmic drums conducive to dancing. Microhouse, a sub-genre blending elements of techno with a slower tempo, features soft, cushiony kick-drums and a dreamy ambiance. The Microhouse tracks included in the corpus draw inspiration from compositions by Zip and Ricardo Villalobos. The Tech House tracks included in the corpus are sourced from compositions by Mr C, Nathan Coles, and Buschwacka!. Tech House is characterized as a fusion genre merging techno and house elements, featuring groove-driven beats, a minimalist approach, and the integration of technological sounds. 

<br>

The selected corpus closely aligns with my musical interests and expertise. As a passionate DJ and music producer with <a href="https://open.spotify.com/artist/1ux6lrKnUuH8POcbD6TzpF?si=Uq9N_4n_QAGJC7MmYbxiMw" target="_blank">releases on Spotify</a>, my focus lies predominantly within the realm of house music. House music, characterized by a diverse spectrum of genres akin to other musical forms, encompasses a multitude of intriguing niches, with distinctions evident across various elements such as arrangement, drum patterns, sampling techniques, and tempo. Particularly within underground house music, which diverges from mainstream popular culture to cater to devoted enthusiasts, the differentiation among these niches is pronounced. Originating in Chicago's underground club scene in the 1980s, underground house music swiftly gained traction in Europe, with the UK actively participating in its evolution. During those years, thousands of vinyl records were pressed, each release typically limited to a small number of copies owing to the niche nature of the genre. Countries such as Germany, Italy, The Netherlands, and Sweden swiftly embraced and enriched the underground house music scene. Even today, at parties and festivals, these vintage records seamlessly intertwine with modern releases, showcasing the timeless qualities of certain productions from earlier years. While the complexity within these niches is undeniable, discussions with fellow composers often center around the perceived simplicity of house music. The question arises: "Isn't house music merely a repetitive 4/4 drum beat coupled with a basic arrangement template (intro, buildup, outro)?". In my perspective, the answer lies in both affirmation and negation. While many tracks categorized as house music exhibit repetitiveness and adhere to a basic arrangement template, the vast distinctions across various musical elements within the niches of underground house music underscore the complexity inherent in the genre. In this research, I aim to utilize the Spotify API as a third party in this discourse, shedding new light on the discussion by providing insights into the intricacies of house music.

<br>

The research examines the following main research question: 'How do the features extracted from the Spotify API contribute to understanding the extent of repetitiveness, complexity, and creative variation within House music as a genre?'. To address this question, the research is divided into several parts, each examining different aspects of music using the features provided by the Spotify API.
</div>
</div>

Track-level Features
=====================================

Row {.tabset .tabset-fade}
-------------------------------------

### Danceability & Valence
<div class="wrapper_single">

<div class="header_wrapper_single">

<div class="header_single">
Danceability & Valence
</div>

<div class="header_question_single">
<p><em>'Is House music classified as 'danceable' by the Spotify API, and is it correlated with its valence?'</em></p>
<hr/>
</div>

</div>

<div class="text_single">
House music is renowned for its universal appeal, primarily driven by its infectious beat. Whether it's the subtle swing of your legs, the rhythmic nodding of your head to the bass kick, or even the tapping of your fingers on a table, the 4x4 beat has a compelling allure that's hard to resist. However, some individuals find themselves bothered by the incessant drumming beat, perceiving it as overly repetitive. Yet, are these assertions accurate? Isn't there a broader spectrum of variability among these genres? Another intriguing aspect to consider is the valence of the track. If a track is highly danceable, does this correlate with a positive emotional tone? It's commonly observed in DJ mixes that the atmosphere on the dancefloor tends to be more upbeat at the beginning compared to the end. Therefore, exploring the correlation between these features across sub-genres would provide valuable insights.

<br>

In the figure provided below, Valence is plotted against Danceability using Spotify API features. The data suggests that most tracks in the House genre demonstrate high danceability, concentrated around 0.8, with more variability at the upper range compared to the lower range. Hence, it is reasonable to conclude that House music possesses a high danceability factor, supporting the hypothesis that House music is inherently danceable. It is worth mentioning that Bass House music exhibits a higher level of danceability compared to other sub-genres. This could be attributed to the frequent use of aggressive kick drums and hats, which enhance the danceable nature of the beat. In contrast, Microhouse tends to feature hats that are more subdued in the background. Surprisingly, there are instances within Tech House where certain tracks score low on danceability, which was unexpected.

<br>

The variance in valence among tracks is higher compared to that of danceability. However, it is worth noting that Microhouse tracks often exhibit lower valence levels compared to other sub-genres. This difference is particularly evident when examining the correlation lines between the two features, with the Microhouse correlation line consistently positioned lower than those of other sub-genres. The correlation values for the Bass House category are 0.019, for the Microhouse category 0.257, and for the Tech House category -0.179. These correlation values are not particularly compelling and therefore do not provide sufficient evidence to claim that the features are strongly correlated within any of the sub-genres. However, the discrepancies between the correlation values do suggest that there are indeed differences in the relationship between danceability and valence across the sub-genres.
</div>

<div class="figure_single">
```{r, echo=FALSE}
bass_house$category <- "Bass House"
tech_house$category <- "Tech House"
microhouse$category <- "Microhouse"

# Combine dataframes into one
combined_df <- rbind(bass_house, tech_house, microhouse)

# Plot the scatter plot
ggplot(combined_df, aes(x = danceability, y = valence, color = category)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Danceability", y = "Valence", color = "Category") +
  theme_minimal()
```



</div>
</div>

### Tempo
<div class="wrapper_single">

<div class="header_wrapper_single">

<div class="header_single">
Tempo
</div>

<div class="header_question_single">
<p><em>'To what extent can the tempo of House music be generalized?'</em></p>
<hr/>
</div>

</div>

<div class="text_single">
House music tracks often exhibit significant variations in BPM, which is understandable given the nature of DJing. In a House music event, the opening DJ typically starts their DJ mix at a lower BPM around 123, while the closing DJ tends to end their DJ mix at a higher BPM, approximately 136. This variability suggests that House music offers a diverse musical landscape. To validate this assertion, Spotify's API will be utilized to analyze the BPM of tracks within different sub-genres, examining whether there are notable differences in BPM ranges among them.

<br>

The violin plots below depict all tracks within the corpora of the sub-genres. It is evident that the Tech House genre encompasses tracks with a higher BPM compared to the other sub-genres. While both Bass House and Microhouse genres exhibit similar BPM ranges, Bass House tends to have a greater deviation towards higher BPM values, whereas Microhouse leans towards lower BPM values. Additionally, the variance of Tech House BPMs is notably higher than that of the other sub-genres. Tech House demonstrates a concentrated cluster around 133 BPM, while Microhouse hovers around 126 BPM and Bass House tends to center around 124 BPM. Therefore, it can be concluded that there are clear differences in tempo among these sub-genres.
</div>

<div class="figure_single">
```{r, echo=FALSE}
categories <-
  bind_rows(
    bass_house |> mutate(category = "Bass House"),
    microhouse |> mutate(category = "Microhouse"),
    tech_house |> mutate(category = "Tech House")
  )

categories |>
  ggplot(aes(x = category, y = tempo)) +
  geom_violin()
```
</div>
</div>

### Duration
<div class="wrapper_single">

<div class="header_wrapper_single">

<div class="header_single">
Duration
</div>

<div class="header_question_single">
<p><em>'To what extent does House music exhibit variability in duration?'</em></p>
<hr/>
</div>

</div>

<div class="text_single">
The duration of tracks is another factor that merits discussion. It's subjective and varies depending on the DJ and their style. Some DJs prefer hour-length sets, while others may extend theirs to six hours or more. Consequently, DJs often select tracks with longer durations to craft mixes that resemble storytelling, rather than focusing solely on playing the most popular tracks of the moment. Hence, it is interesting to visualize the durations of the tracks within the corpus.

<br>

The violin plots below illustrate all tracks within the corpus of the sub-genres. The figures indicate that Microhouse tracks frequently exhibit longer durations compared to other sub-genres. However, Microhouse also displays higher variability in duration compared to the other genres, particularly in contrast to the condensed nature of Bass House. Conversely, Tech House demonstrates more variability in duration.
</div>

<div class="figure_single">
```{r, echo=FALSE}
categories <-
  bind_rows(
    bass_house |> mutate(category = "Bass House"),
    microhouse |> mutate(category = "Microhouse"),
    tech_house |> mutate(category = "Tech House")
  )

categories |>
  ggplot(aes(x = category, y = track.duration_ms)) +
  geom_violin()
```
</div>
</div>

### Loudness
<div class="wrapper_single">

<div class="header_wrapper_single">

<div class="header_single">
Loudness
</div>

<div class="header_question_single">
<p><em>'Has House music experienced a trend towards increased loudness?'</em></p>
<hr/>
</div>

</div>

<div class="text_single">
House music has its roots in analogue signals and physical machines, where musicians crafted music by manipulating knobs and recording their performances live onto vinyl, cassettes, and CDs. With the advent of Digital Audio Workstations (DAWs), music production has become more refined both structurally and in terms of mastering. Tracks undergo multiple mastering processes to ensure optimal sound quality for live performances in concerts and clubs. As the dB levels in clubs continue to rise, it prompts questions about whether this trend correlates with an increase in the loudness of tracks over the years. If so, could the Spotify API prove useful in visualizing this trend?

<br>

In the figure below, the release dates of tracks are plotted against their loudness values obtained from the Spotify API. It's evident that Tech House tracks released around 2001 exhibit lower loudness values compared to those released from 2014 onwards. The loudness of Microhouse tracks has remained relatively constant, although there are instances of tracks with lower loudness values, particularly in 2019. On the other hand, tracks from the Bass House genre show a consistent loudness level across releases, suggesting a uniformity in their sound profile from a specific starting point.

<br>

Analysis of the figures reveals a noticeable trend towards increased loudness in Tech House music. Conversely, the loudness of other sub-genres appears to remain relatively constant, which may partly be attributed to their respective release periods. It's worth noting that during the time period when these tracks were originally released, music production primarily involved DAWs and frequent mastering. Consequently, one could argue that House music, as a whole, has undergone a trend towards heightened loudness over time. It is also noteworthy that artists occasionally remaster their old music as part of the original project, re-releasing it on platforms like Spotify. This process often results in a noticeable increase in the loudness of the tracks. This could potentially explain the observed trend of increased loudness in Tech House tracks, especially those that have been recently released, as some of them may be remastered versions of older tracks. 
</div>

<div class="figure_single">
```{r, echo=FALSE}
# Assign categories
bass_house$category <- "Bass House"
tech_house$category <- "Tech House"
microhouse$category <- "Microhouse"

combined_df$track.album.release_date <- as.Date(combined_df$track.album.release_date)

# Plot the scatter plot
ggplot(combined_df, aes(x = track.album.release_date, y = loudness, color = category)) +
  geom_point() +
  labs(x = "Release date", y = "Loudness", color = "Category") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year", limits = as.Date(c("1995-01-01", "2023-12-31"))) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```
</div>
</div>

Pitch
=====================================

<div class="wrapper">

<div class="header_wrapper_single">

<div class="header_single">
Chromagrams
</div>

<div class="header_question_single">
<p><em>'To what extent do chroma features offer insights into the chordal diversity across House music'</em></p>
<hr/>
</div>

</div>

<div class="text">
One characteristic of repetitiveness in music is the consistency of pitch within a given track. This aspect can vary across tracks, with some emphasizing chord progressions while others highlight percussive elements. To analyze this aspect across tracks in the corpus, a chromagram will be generated for each representative track to assess the constancy of pitch throughout its duration. For the Bass House genre, a track by Samuel Deep has been selected, primarily featuring a prominent bass kick, basslines, percussive elements, and glitches. The Microhouse track is characterized by its gradual evolution and mysterious atmosphere, sustained by a constant ambient string in the background. Initially subtle, this string presence becomes increasingly pronounced as the track progresses. In contrast, the Tech House track showcases a repetitive chord progression that serves as a background motif, adapting across different musical sections of the track.

<br>

The figures below illustrate a consistent pitch pattern observed across the tracks. Each track predominantly centers around a singular key, with some deviations. In the track by Samuel Deep, the focus is primarily on the notes C and B, with a shift to F# occurring approximately 180 seconds into the track. Priku's track centers around the key of D, with notable alterations occurring at 120, 270, and 350 seconds. Voigtman's track primarily revolves around the key of C#, with transitions to E occurring at approximately 180 and 210 seconds.

<br>

Upon analyzing the chromagrams, it can be observed that House tracks do not heavily rely on specific pitch classes. Furthermore, it is evident that there are discernible differences among sub-genres. Microhouse shows greater chord variation compared to Bass House and Tech House. These findings suggest that Microhouse is more flexible in chord structuring than other sub-genres. Thus, based on chroma features, there are notable differences in how sub-genres structure their chords within tracks. Consequently, it can be concluded that chroma features provide valuable insights into the chordal diversity across House music.
</div>
<div class="figures">
<div class="figure">
```{r, echo=FALSE}
wood <-
  get_tidy_audio_analysis("5q7X58mUQFEuQ0sG9xx1L8") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)


wood |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Bass House: 'Samuel Deep - 3521'") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```
</div>
<div class="figure">
```{r, echo=FALSE}
wood <-
  get_tidy_audio_analysis("0qvhTKjwlB7vMzzeSTuMnQ") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)


wood |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Microhouse: 'Priku - Melodic'") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```
</div>
<div class="figure">
```{r, echo=FALSE}
wood <-
  get_tidy_audio_analysis("62ISyEdD2PnXo4d2HH8qS9") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)


wood |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Tech House: 'Voigtmann - Emu Emu'") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```
</div>
</div>
</div>

<!-- ### Dynamic Time Warping -->
<!-- <div class="wrapper"> -->
<!-- <div class="headr"> -->
<!-- Dynamic Time Warping -->
<!-- </div> -->
<!-- <div class="text"> -->
<!-- These visuals exhibit the dynamic time warping of three tracks from the corpus. -->
<!-- </div> -->
<!-- <div class="figures"> -->
<!-- <div class="figure"> -->
<!-- ```{r, echo=FALSE} -->
<!-- # ## Electro: Setaoc Mass - Disrepair -->
<!-- setaoc <- -->
<!--   get_tidy_audio_analysis("1N1cKosA1wPzPEydz9m625") |> -->
<!--   select(segments) |> -->
<!--   unnest(segments) |> -->
<!--   select(start, duration, pitches) -->
<!-- ## Minimal: Petre Inspirescu - Sakadat -->
<!-- sakadat <- -->
<!--   get_tidy_audio_analysis("7hERmmf3Srd7Jb0JVFHl2a") |> -->
<!--   select(segments) |> -->
<!--   unnest(segments) |> -->
<!--   select(start, duration, pitches) -->

<!-- compmus_long_distance( -->
<!--   setaoc |> mutate(pitches = map(pitches, compmus_normalise, "chebyshev")), -->
<!--   sakadat |> mutate(pitches = map(pitches, compmus_normalise, "chebyshev")), -->
<!--   feature = pitches, -->
<!--   method = "euclidean" -->
<!-- ) |> -->
<!--   ggplot( -->
<!--     aes( -->
<!--       x = xstart + xduration / 2, -->
<!--       width = xduration, -->
<!--       y = ystart + yduration / 2, -->
<!--       height = yduration, -->
<!--       fill = d -->
<!--     ) -->
<!--   ) + -->
<!--   geom_tile() + -->
<!--   ggtitle("House") + -->
<!--   coord_equal() + -->
<!--   labs(x = "Brawther - Basix (Deep Mix)", y = "William Caycedo - Roasted (Original Mix)") + -->
<!--   theme_minimal() + -->
<!--   scale_fill_viridis_c(guide = NULL) -->
<!-- ``` -->
<!-- </div> -->
<!-- <div class="figure"> -->
<!-- ```{r, echo=FALSE} -->
<!-- # ## Electro: Setaoc Mass - Disrepair -->
<!-- # setaoc <- -->
<!-- #   get_tidy_audio_analysis("5JcTPYvJmKbgQa9tffD3fg") |> -->
<!-- #   select(segments) |> -->
<!-- #   unnest(segments) |> -->
<!-- #   select(start, duration, pitches) -->
<!-- # ## Minimal: Petre Inspirescu - Sakadat -->
<!-- # sakadat <- -->
<!-- #   get_tidy_audio_analysis("5GDnFk9M3qPlTKY4XIgYdl") |> -->
<!-- #   select(segments) |> -->
<!-- #   unnest(segments) |> -->
<!-- #   select(start, duration, pitches) -->
<!-- #  -->
<!-- # compmus_long_distance( -->
<!-- #   setaoc |> mutate(pitches = map(pitches, compmus_normalise, "chebyshev")), -->
<!-- #   sakadat |> mutate(pitches = map(pitches, compmus_normalise, "chebyshev")), -->
<!-- #   feature = pitches, -->
<!-- #   method = "euclidean" -->
<!-- # ) |> -->
<!-- #   ggplot( -->
<!-- #     aes( -->
<!-- #       x = xstart + xduration / 2, -->
<!-- #       width = xduration, -->
<!-- #       y = ystart + yduration / 2, -->
<!-- #       height = yduration, -->
<!-- #       fill = d -->
<!-- #     ) -->
<!-- #   ) + -->
<!-- #   geom_tile() + -->
<!-- #   ggtitle("Minimal") + -->
<!-- #   coord_equal() + -->
<!-- #   labs(x = "iO (Mulen) - Stick Out", y = "Ricardo Villalobos - Logohitz") + -->
<!-- #   theme_minimal() + -->
<!-- #   scale_fill_viridis_c(guide = NULL) -->
<!-- ``` -->
<!-- </div> -->
<!-- <div class="figure"> -->
<!-- ```{r, echo=FALSE} -->
<!-- # ## Electro: Setaoc Mass - Disrepair -->
<!-- # setaoc <- -->
<!-- #   get_tidy_audio_analysis("4Co3Fmz3EpfiQE3U5HfoMY") |> -->
<!-- #   select(segments) |> -->
<!-- #   unnest(segments) |> -->
<!-- #   select(start, duration, pitches) -->
<!-- # ## Minimal: Petre Inspirescu - Sakadat -->
<!-- # sakadat <- -->
<!-- #   get_tidy_audio_analysis("5oisJqdx3cXLCd9H5Xdvzc") |> -->
<!-- #   select(segments) |> -->
<!-- #   unnest(segments) |> -->
<!-- #   select(start, duration, pitches) -->
<!-- #  -->
<!-- # compmus_long_distance( -->
<!-- #   setaoc |> mutate(pitches = map(pitches, compmus_normalise, "chebyshev")), -->
<!-- #   sakadat |> mutate(pitches = map(pitches, compmus_normalise, "chebyshev")), -->
<!-- #   feature = pitches, -->
<!-- #   method = "euclidean" -->
<!-- # ) |> -->
<!-- #   ggplot( -->
<!-- #     aes( -->
<!-- #       x = xstart + xduration / 2, -->
<!-- #       width = xduration, -->
<!-- #       y = ystart + yduration / 2, -->
<!-- #       height = yduration, -->
<!-- #       fill = d -->
<!-- #     ) -->
<!-- #   ) + -->
<!-- #   geom_tile() + -->
<!-- #   ggtitle("Electro") + -->
<!-- #   coord_equal() + -->
<!-- #   labs(x = "Ludwig A.F. - First Flight", y = "D.I.E. - Programming") + -->
<!-- #   theme_minimal() + -->
<!-- #   scale_fill_viridis_c(guide = NULL) -->
<!-- ``` -->
<!-- </div> -->
<!-- </div> -->
<!-- </div> -->


Timbre Features
=====================================
<div class="wrapper">

<div class="header_wrapper_single">

<div class="header_single">
Self-Similarity Matrices
</div>

<div class="header_question_single">
<p><em>'Does House music often show the same musical structure?'</em></p>
<hr/>
</div>

</div>

<div class="text">

Music frequently adheres to distinct structural frameworks, such as the sonata form, ternary form, or rondo form. In less mainstream genres, artists often have more freedom to experiment with alternative structures. The question arises: does this freedom extend to House music, or do its sub-genres exhibit repetitive musical structures? To investigate this, self-similarity matrices will be employed.

<br>

The figures below display the Self-Similarity Matrices of three tracks within the corpus based on their timbre features. In the Bass House track, a build-up around 155 seconds is visible. Additionally, numerous subtle changes are apparent, possibly indicating shifts in percussion, a characteristic of Stooge Wilson's production style. In the Microhouse track, constant fluctuations are observed, stemming from its repetitive melody. The previously noted build-up around 320 seconds is less pronounced, but both the intro is clearly depicted in this figure. In the Tech House track, the repetition in organization not evident, marking an experimental musical structure. Therefore it's popularity is more sparser, and people quickly see House music as one genre.

<br>

When comparing the matrices, a clear generalizable musical structure isn't evident. The matrices seem to display a more experimental structure, deviating from a standard Intro – Verse – Chorus – Verse – Chorus – Bridge – Chorus – Outro. Therefore, it can be concluded that House music deviates from predefined musical structures and allows for more experimental freedom.

</div>
<div class="figures">
<div class="figure">
```{r, echo=FALSE}
bzt_house2 <-
  get_tidy_audio_analysis("30TlI5a7vVhqRiTGwOr7Gb") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

bzt_house2 |>
  compmus_self_similarity(timbre, "cosine") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  ggtitle("Bass House: 'Stooge Wilson - Nxsty'") +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```
</div>
<div class="figure">
```{r, echo=FALSE}
bzt_minimal2 <-
  get_tidy_audio_analysis("0hUMQoiYa1p0XgVmlCBcd0") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

bzt_minimal2 |>
  compmus_self_similarity(timbre, "cosine") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  ggtitle("Microhouse: 'Korsakow - Make You Crazy'") +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```
</div>
<div class="figure">
```{r, echo=FALSE}
bzt_electro2 <-
  get_tidy_audio_analysis("7GNuJCJRlSpKxvyWwzalXt") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

bzt_electro2 |>
  compmus_self_similarity(timbre, "cosine") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  ggtitle("Tech House: 'Dexter - Fantasia'") +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```
</div>
</div>
</div>

Temporal Features
=====================================

Row {.tabset .tabset-fade}
-------------------------------------

### Novelty Functions
<div class="wrapper">

<div class="header_wrapper_single">

<div class="header_single">
Novelty Functions
</div>

<div class="header_question_single">
<p><em>'How accurately is house music's consistent beat perception reflected in the actual frequency of energy shifts within tracks?'</em></p>
<hr/>
</div>

</div>

<div class="text">
House music is often characterized by a consistent beat that persists throughout the entire track. However, is this characterization accurate, or does the genre contain more frequent shifts in energy than commonly perceived? One way to analyze this is by examining the novelty of a track. Using the Spotify API, the loudness of each track can be obtained, providing insight into its novelty.

<br>

In the figures below, Novelty Functions for one track from each of the sub-genres are plotted. Larry de Kat's track exhibits several novelty spikes but generally maintains lower novelty compared to the other tracks. Tom Ellis's track demonstrates more frequent and higher novelty peaks throughout its duration. Similarly, Nathan Coles's track also exhibits frequent peaks, albeit with slightly lower intensity.

<br>
The novelty function indicates a level of variation in loudness across the figures. However, there are some doubts regarding the representativeness of the novelty functions. Upon personally listening to the tracks, I struggle to align my experience with the notion of constant novelty in loudness as suggested by the figures.
</div>

<div class="figures">
<div class="figure">

```{r, echo=FALSE}
pata_pata <-
  get_tidy_audio_analysis("4uAxqp5rBBvm6eKXuAavIZ") |>
  select(segments) |>
  unnest(segments)

pata_pata |>
  mutate(loudness_max_time = start + loudness_max_time) |>
  arrange(loudness_max_time) |>
  mutate(delta_loudness = loudness_max - lag(loudness_max)) |>
  ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
  ggtitle("Bass House: 'Larry de Kat - Def Joint'") +
  geom_line() +
  xlim(0, 30) +
  theme_minimal() +
  labs(x = "Time (s)", y = "Novelty")
```
</div>

<div class="figure">

```{r, echo=FALSE}
pata_pata <-
  get_tidy_audio_analysis("470lb3XFRIOFT7pb19fKni") |>
  select(segments) |>
  unnest(segments)

pata_pata |>
  mutate(loudness_max_time = start + loudness_max_time) |>
  arrange(loudness_max_time) |>
  mutate(delta_loudness = loudness_max - lag(loudness_max)) |>
  ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
  ggtitle("Microhouse: 'Tom Ellis - Yah Yah'") +
  geom_line() +
  xlim(0, 30) +
  theme_minimal() +
  labs(x = "Time (s)", y = "Novelty")
```
</div>

<div class="figure">

```{r, echo=FALSE}
pata_pata <-
  get_tidy_audio_analysis("7LHbSCq1bnwfAdeAvTtHgA") |>
  select(segments) |>
  unnest(segments)

pata_pata |>
  mutate(loudness_max_time = start + loudness_max_time) |>
  arrange(loudness_max_time) |>
  mutate(delta_loudness = loudness_max - lag(loudness_max)) |>
  ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
  ggtitle("Tech House: 'Nathan Coles - Hide & Seek'") +
  geom_line() +
  xlim(0, 30) +
  theme_minimal() +
  labs(x = "Time (s)", y = "Novelty")
```
</div>
</div>
</div>

### Tempograms
<div class="wrapper">

<div class="header_wrapper_single">

<div class="header_single">
Tempograms
</div>

<div class="header_question_single">
<p><em>'To what extent can the tempo of House music be generalized?'</em></p>
<hr/>
</div>

</div>

<div class="text">
Alterations in tempo within House music tracks are infrequent; nearly every track maintains a constant BPM level from start to finish. This consistency arises from the fact that these tracks are typically not performed live, and the tempo is uniformly set across all elements. Consequently, abrupt shifts in BPM are generally considered unfavorable. In DJ mixes, tempo changes are indeed present, but they are executed gradually using turntables throughout the set. To investigate this popular constant tempo in House music, the Spotify API will be utilized to generate tempograms and assess whether the tempo remains consistent throughout the duration of the tracks.

<br>

The figures presented depict the Tempograms of three tracks from the corpus, each demonstrating minimal variation in tempo. However, in the Bass House track, the Spotify API identifies a noticeable shift in tempo, though it inaccurately interprets this as a significant change when it is merely a slight buildup within the track. The Microhouse track exhibits a consistent tempo with some background noise evident. Additionally, the track incorporates sound effects of people talking and ambient noise from a restaurant kitchen, which likely interferes with the accuracy of the tempo detected by the Spotify API, resulting in a less distinct tempo line. The Tech House track maintains a constant tempo throughout its duration. Notably, the bass drum remains consistently prominent throughout the track, contrasting with the Bass House track. This characteristic may contribute to the Spotify API's ability to make a strong assumption about the tempo of the Tech House track. All three tracks closely hover around 126 BPM. These figures demonstrate that the Spotify API perceives House music as a genre characterized by tracks with a consistently stable tempo, all hovering around the same BPM.
</div>

<div class="figures">
<div class="figure">

```{r, echo=FALSE}
graveola <- get_tidy_audio_analysis("7eiYOdSebHEUU4sQfaifLS")

graveola |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle("Bass House: 'Julien Fuentes, Borren - Seabert'") +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

</div>

<div class="figure">

```{r, echo=FALSE}
graveola <- get_tidy_audio_analysis("30o5OLtfYd1MBLoGC85N6A")

graveola |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle("Microhouse: 'Plusculaar - Rhadoo Le (Original Mix)'") +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

</div>

<div class="figure">

```{r, echo=FALSE}
graveola <- get_tidy_audio_analysis("15NAPmf13bSFdH3psDdxTU")

graveola |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle("Tech House: 'Terry Francis - Hannah's Dub'") +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

</div>
</div>
</div>

Classifier Algorithm
=====================================
<div class="wrapper_single">

<div class="header_wrapper_single">

<div class="header_single">
Classifier Algorithm
</div>

<div class="header_question_single">
<p><em>'What critical features differentiate sub-genres within the House music genre?'</em></p>
<hr/>
</div>

</div>

<div class="text_single">
Another interesting approach to explore potential differences among the three sub-genres is by assessing whether a model can successfully differentiate between them. If the model shows the ability to distinguish between the sub-genres, it provides further significance to the research question. The model to be utilized is a classifier known as k-nearest neighbors (KNN). This algorithm operates as a supervised learning classifier, predicting the classification of individual data points. To adapt KNN for investigating potential differences among the sub-genres, the tracks within the corpora are represented as data points, with the three sub-genres representing distinct groupings. To train the KNN, the data points are divided into two groups: training data and test data. Since the classifier is trained on the labeled training data, which already defines the grouping of each data point, KNN operates as a supervised algorithm. After training the model, the model will make its predictions on the test data. Based on these predictions, several key metrics are established to evaluate the model's performance. True Positive (TP) indicates the correct classification of positive instances, False Positive (FP) indicates the incorrect classification of negative instances, True Negative (TN) indicates the correct classification of negative instances, and False Negative (FN) indicates the incorrect of positive instances. These metrics serve as vital tools in assessing the accuracy and effectiveness of KNN classification, offering insights into where the model may flucuate in its predictions. After training the KNN classifier on the corpora, the following predictions are made. When looking at the diagonal line in the figure there can be seen that model is able to make correct predictions. A clearer distinction between Bass House and Tech House can be seen compared to Microhouse. Microhouse has 6 True positives whereas Bass House and Tech House have 6 and 9 tracks, respectively. For Bass House the precision and recall scores are 0.86 and 0.90 (respectively), for Microhouse the precision and recall scores are 0.83 and 0.75 (respectively), and for Tech House the precision and recall scores are 0.81 and 0.85 (respectively).
</div>

<div class="figure_single">
```{r, echo=FALSE}
indie <-
  bind_rows(
    bass_house |> mutate(playlist = "Bass House") |> slice_head(n = 10),
    tech_house |> mutate(playlist = "Tech House") |> slice_head(n = 10),
    microhouse |> mutate(playlist = "Microhouse") |> slice_head(n = 10)
  ) |>
  add_audio_analysis()

indie_features <-
  indie |>  # For your portfolio, change this to the name of your corpus.
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

indie_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration,
    data = indie_features           # Use the same name as the previous block.
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].

indie_cv <- indie_features |> vfold_cv(5)

knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |>
  set_engine("kknn")
indie_knn <-
  workflow() |>
  add_recipe(indie_recipe) |>
  add_model(knn_model) |>
  fit_resamples(indie_cv, control = control_resamples(save_pred = TRUE))

# indie_knn |> get_conf_mat()

indie_knn |> get_conf_mat() |> autoplot(type = "heatmap")
```
</div>

<div class="text_single">
To train the classifier, 10 features from the corpera are utilized. Optimizing the classifier may involve reducing the number of features to enhance training efficiency. By employing a random forest model, the features that exert the greatest influence on the grouping of data points can be identified. Upon applying random forest to the corpera, the following order of features are observed, indicating their influence on data point groupings. Notably, track duration and tempo exhibit the highest influence, while acousticness, instrumentalness, and liveness demonstrate minimal impact on the groupings.
</div>

<div class="figure_single">
```{r, echo=FALSE}
forest_model <-
  rand_forest() |>
  set_mode("classification") |>
  set_engine("ranger", importance = "impurity")
indie_forest <-
  workflow() |>
  add_recipe(indie_recipe) |>
  add_model(forest_model) |>
  fit_resamples(
    indie_cv,
    control = control_resamples(save_pred = TRUE)
  )

# indie_forest |> get_pr()

workflow() |>
  add_recipe(indie_recipe) |>
  add_model(forest_model) |>
  fit(indie_features) |>
  pluck("fit", "fit", "fit") |>
  ranger::importance() |>
  enframe() |>
  mutate(name = fct_reorder(name, value)) |>
  ggplot(aes(name, value)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")
```
</div>

<div class="text_single">
If the classifier is trained again on the corpara but instead only with the 'tempo' and 'duration' features, the following figure is obtained. What is interesting to notice here is that Microhouse now has a higer rate of True positives compared to Bass House and Tech House. For Bass House the precision and recall scores are now 0.77 and 0.85 (respectively), for Microhouse the precision and recall scores are 0.85 and 0.85 (respectively), and for Tech House the precision and recall scores are 0.83 and 0.75 (respectively).
</div>

<div class="figure_single">
```{r, echo=FALSE}
indie <-
  bind_rows(
    bass_house |> mutate(playlist = "Bass House") |> slice_head(n = 10),
    tech_house |> mutate(playlist = "Tech House") |> slice_head(n = 10),
    microhouse |> mutate(playlist = "Microhouse") |> slice_head(n = 10)
  ) |>
  add_audio_analysis()

indie_features <-
  indie |>  # For your portfolio, change this to the name of your corpus.
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

indie_recipe <-
  recipe(
    playlist ~
      tempo +
      duration,
    data = indie_features           # Use the same name as the previous block.
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].

indie_cv <- indie_features |> vfold_cv(5)

knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |>
  set_engine("kknn")
indie_knn <-
  workflow() |>
  add_recipe(indie_recipe) |>
  add_model(knn_model) |>
  fit_resamples(indie_cv, control = control_resamples(save_pred = TRUE))

# indie_knn |> get_conf_mat()

indie_knn |> get_conf_mat() |> autoplot(type = "heatmap")
```
</div>


</div>





Conclusion {.intro-text}
=====================================
<div class="wrapper">
<div class="headr">
Conclusion
</div>
<div class="text">
The analysis of various sub-genres within House music reveals distinct trends across different features. Microhouse tracks consistently show lower valence levels compared to others, while Tech House stands out with a higher BPM range and greater variability. Additionally, Microhouse tracks tend to have longer durations with higher variability, contrasting with the condensed nature of Bass House. A noticeable trend towards increased loudness is observed in Tech House, possibly due to remastering practices, while other sub-genres maintain relatively constant loudness levels. House music's deviation from predefined musical structures allows for experimental freedom, reflected in the data's lack of clear generalizable patterns. Doubts arise regarding the representativeness of novelty functions, indicating potential discrepancies between data analysis and subjective listening experience. Furthermore, House music generally maintains a stable tempo, with slight variations detected, especially in Bass House tracks. Utilizing a random forest model, features like track duration and tempo exert the most influence on data grouping, highlighting the diverse characteristics of different House music sub-genres across multiple dimensions and contributing to a deeper understanding of the genre's nuances.

<br>

These findings suggest that the Spotify API contributes significantly to the discourse by providing valuable information on all the tracks in the corpus. Based on its findings, one could argue that House music transcends the typical 4x4 beat and encompasses deeper layers, although this may not apply universally across all House music. This portfolio has only considered three sub-genres of House music, yet there are numerous other sub-genres. Additionally, the corpus contains only a small percentage of tracks representing these sub-genres. However, a notable aspect of this corpus is that many tracks setting the foundation for its characteristics are primarily available on vinyl and not on Spotify. While some tracks have been remastered, this does not fully address this limitation.

<br>

Another important aspect of House music is its primary purpose: to be experienced on the dancefloor rather than being repeatedly streamed on headphones. On the dancefloors, DJs aim to convey a narrative through their track selection rather than compiling a diverse playlist. DJs in this realm meticulously curate their record collections for their DJ mixes, showcasing them selectively. Unlike Spotify, there isn't an algorithm in the House music scene that recommends new tracks based on listening history or co-listeners. In conclusion, the examination of underground House music may present challenges due to its exclusivity. However, despite this, insights about the genre were still obtained through the availability of current remasters on Spotify. Is underground House music more than a typical 4x4 beat? Undoubtedly, from the analysis and also supported by data from the Spotify API. And when you are still not convinced, you should come to the dance floor once.

<br>

For future research in the realm of House music. one could delve deeper into exploring the evolving landscape of sub-genres and their impact on cultural and artistic expressions. Investigating the emergence of new sub-genres and their resonance within the global electronic music community could provide valuable insights into shifting trends and audience preferences. Additionally, examining the influence of technological advancements, such as streaming platforms and digital production tools, on the production, distribution, and consumption of House music could offer a comprehensive understanding of its contemporary relevance. This research would be interesting to musicians, DJs, practitioners in musicology through providing them with nuanced perspectives on the ongoing evolution of electronic dance music and its socio-cultural significance in the digital age.
</div>
</div>


Irrelevant figures{.intro-text}
=====================================

Row {.tabset .tabset-fade}
-------------------------------------

### Cepstrograms
<div class="wrapper">

<div class="header_wrapper_single">

<div class="header_single">
Cepstrograms
</div>

<div class="header_question_single">
<p><em></em></p>
<hr/>
</div>

</div>

<div class="text">
These figures depict the cepstrograms of three tracks within the corpus. They didn't seem compelling for this portfolio.
</div>
<div class="figures">
<div class="figure">
```{r, echo=FALSE}
bzt_house <-
  get_tidy_audio_analysis("2UYmW0F3ZvdplGyz2ZWH9N") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

bzt_house |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Bass House: 'Locklead - Keep Going'") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +
  theme_classic()
```
</div>
<div class="figure">
```{r, echo=FALSE}
bzt_minimal <-
  get_tidy_audio_analysis("596xBkksPNU9VYgBeC6jND") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

bzt_minimal |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Microhouse: 'Duky - Endless Thoughts'") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +
  theme_classic()
```
</div>
<div class="figure">
```{r, echo=FALSE}
bzt_electro <-
  get_tidy_audio_analysis("3gAVzxCCo8zNlZDYsseJq7") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

bzt_electro |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Tech House: '100Hz - Stay'") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +
  theme_classic()
```
</div>
</div>
</div>

### Chordograms
<div class="wrapper">
<div class="headr">
Chordograms
</div>
<div class="text">
These figures depict the cepstrograms of three tracks within the corpus. They didn't seem compelling for this portfolio.
</div>

<div class="figures">
<div class="figure">

```{r, echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

twenty_five <-
  get_tidy_audio_analysis("4TRuGdaiT2qoko85097hKq") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

twenty_five |>
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  ggtitle("Bass House: 'Julian Alexander - Sagacious'") +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```
</div>

<div class="figure">

```{r, echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

twenty_five <-
  get_tidy_audio_analysis("74j9d4tz6Ixw8pkkA1RCCM") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

twenty_five |>
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  ggtitle("Microhouse: 'Bodeler - Philypop'") +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```
</div>

<div class="figure">

```{r, echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

twenty_five <-
  get_tidy_audio_analysis("5iOk4ZzlRRGguhzzRvGkS4") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

twenty_five |>
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  ggtitle("Tech House: 'Jay Tripwire - Mary Wana'") +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```
</div>
</div>
</div>

### Keygrams
<div class="wrapper">

<div class="headr">
Keygrams
</div>
<div class="text">
These figures depict the keygrams of three tracks within the corpus. They didn't seem compelling for this portfolio.
</div>

<div class="figures">
<div class="figure">

```{r, echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

twenty_five <-
  get_tidy_audio_analysis("4yHUFCUq8DoWxpWStqJQmZ") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

twenty_five |>
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  ggtitle("Bass House: 'U Know The Drill - Back On It'") +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```
</div>

<div class="figure">

```{r, echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

twenty_five <-
  get_tidy_audio_analysis("5QWUSJYczCcxorDYqvhAjg") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

twenty_five |>
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  ggtitle("Microhouse: 'Afrique - Slap'") +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```
</div>

<div class="figure">

```{r, echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

twenty_five <-
  get_tidy_audio_analysis("5WSpFtYT3C065lSMlFln2o") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

twenty_five |>
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  ggtitle("Tech House: 'Chris Jackson - Feel Free'") +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```
</div>
</div>
</div>
